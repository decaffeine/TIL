# 180605 

#### ChainMapper, ChainReducer  
지금까지는 하나의 map과 reducer만 사용했었지만  
Mapper1 -> Mapper2 -> Reducer  
Mapper1 -> Reducer -> Mapper2  
처럼 사용이 가능하다.  

---

### 정렬  
Hadoop은 보조 정렬, 부분 정렬, 전체 정렬을 제공  

### 새 프로젝트 : SortDelayCount

```
DateKey.java : 복합키 정의 
DateKeyComparator.java : 복합키의 비교기(comparator)
GroupKeyPartitioner.java : Grouping Key를 Partitioning  

SortDelayCountMapper.java : Mapper
SortDealyCountReducer.java : Reducer
SortDelayCount.java : Driver

AirlinePerformanceParser.java
```

- DateKey.java  
```java
package com.sist;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableUtils;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

public class DateKey implements WritableComparable<DateKey> {

	private String year;
	private Integer month;
	
	public DateKey() {}
	
	public DateKey(String year, Integer month) {
		super();
		this.year = year;
		this.month = month;
	}

	
	@Override
	public String toString() {
		return year + ", " + month ;
	}

	public String getYear() {
		return year;
	}

	public void setYear(String year) {
		this.year = year;
	}

	public Integer getMonth() {
		return month;
	}

	public void setMonth(Integer month) {
		this.month = month;
	}

	@Override
	public void readFields(DataInput in) throws IOException {
		year = WritableUtils.readString(in);
		month = in.readInt();
	}

	@Override
	public void write(DataOutput out) throws IOException {
		WritableUtils.writeString(out, year);
		out.writeInt(month);		
	}

	@Override
	public int compareTo(DateKey key) {
		
		int result = year.compareTo(key.year);
		if(result == 0) {
			result = month.compareTo(key.month);
		}
		return result;
	}


}
```
  
- DateKeyComparator.java  
```java
package com.sist;

import org.apache.hadoop.io.WritableComparator;
import org.apache.hadoop.io.WritableComparable;

public class DateKeyComparator extends WritableComparator {
	
	protected DateKeyComparator() {
		super(DateKey.class, true);
	}

	@Override
	public int compare(WritableComparable w1, WritableComparable w2) {
		
		DateKey k1 = (DateKey) w1;
		DateKey k2 = (DateKey) w2;
		
		//연도 비교
		int cmp = k1.getYear().compareTo(k2.getYear());
		if(cmp != 0) {
			return cmp;
		}
		
		//월 비교 (같으면 0, k1이 크면 1, k2가 크면 -1)
		return (k1.getMonth() == k2.getMonth()?0:(k1.getMonth()< k2.getMonth())?-1:1);

	}//--compare
	
	

}
```
   
- GroupKeyPartitioner.java  
  
Map task의 출력 data를 reduce task의 입력 data로 보낼지 결정  
이렇게 partitioning한 data는 map task의 출력 data의 key 값에 따라 정렬함  
연도로 partitioning  

```java
package com.sist;

import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.io.IntWritable;

//Partitioner<DateKey, IntWritable>
//map의 출력 data의 key와 value

public class GroupKeyPartitioner extends Partitioner<DateKey, IntWritable> {

	@Override
	public int getPartition(DateKey key, IntWritable val, int numPartitions) {
		int hash = key.getYear().hashCode();
		
		int partition = hash % numPartitions;

		return partition;
	}

}
```

- GroupKeyComparator.java  

```java
package com.sist;

import org.apache.hadoop.io.WritableComparator;
import org.apache.hadoop.io.WritableComparable;
//Reducer 같은 연도에 해당하는 모든 데이터를 처리
public class GroupKeyComparator extends WritableComparator {
	
	protected GroupKeyComparator() {
		super(DateKey.class, true);
	}

	@Override
	public int compare(WritableComparable w1, WritableComparable w2) {
		DateKey k1 = (DateKey) w1;
		DateKey k2 = (DateKey) w2;
		
		return k1.getYear().compareTo(k2.getYear());	

	}
	
	

}

```

- SortDelayCountMapper.java

```java
package com.sist;

import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.io.LongWritable;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

public class SortDelayCountMapper extends Mapper<LongWritable, Text, DateKey, IntWritable> {

	private final static IntWritable one = new IntWritable(1);
	
	private DateKey outputKey = new DateKey();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)
		throws IOException, InterruptedException
	{
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
			if(parser.isDepartureDelayAvailable()){
				if (parser.getDepartureDelayTime() > 0) { //출발지연
					outputKey.setYear("D," + parser.getYear());
					outputKey.setMonth(parser.getMonth());
					
					context.write(outputKey, one);
				}else if (parser.getDepartureDelayTime() == 0){ //정시 출발
					context.getCounter(DelayCounters.scheduled_departure).increment(1);
				}else if (parser.getDepartureDelayTime() < 0) { //일찍 출발
					context.getCounter(DelayCounters.early_departure).increment(1);
				} 
			} else { //출발 지연이 NA
				context.getCounter(DelayCounters.not_available_departure).increment(1);
			}
			
			
 //도착지연
			if(parser.isArriveDelayAvailable()) {
				if (parser.getArriveDelayTime() > 0) {
					outputKey.setYear("A," + parser.getYear());
					outputKey.setMonth(parser.getMonth());
					context.write(outputKey, one);
				} else if (parser.getArriveDelayTime() == 0){
					context.getCounter(DelayCounters.scheduled_arrival).increment(1);
				} else if (parser.getArriveDelayTime() < 0) {
					context.getCounter(DelayCounters.early_arrival).increment(1);
				}
			
			} else {
				context.getCounter(DelayCounters.not_available_arrival).increment(1);
			}
		
	
		
		
		}
	
	
	
}

```

- SortDelayCountReducer.java
```java
package com.sist;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

public class SortDelayCountReducer extends  Reducer<DateKey, IntWritable, DateKey, IntWritable> {

	private IntWritable result = new IntWritable();

	private DateKey outputKey = new DateKey();
	private MultipleOutputs<DateKey, IntWritable> mos;
	

	@Override
	protected void setup(Context context)
			throws IOException, InterruptedException {
		mos = new MultipleOutputs<DateKey, IntWritable>(context);
		
	}//--setup

	//D,1987,02,{1,1,1,1,1}
	//A,1988,01,{1,1,1,1}
	@Override
	protected void reduce(DateKey key, Iterable<IntWritable> values,
			Context context) throws IOException, InterruptedException {
		
		//data 구분자로 분리
		String[] columns = key.toString().split(",");
		
		//outputKey.set(columns[1]+","+columns[2]); //1987,02

		int sum = 0;
		Integer bMonth = key.getMonth();
		
		if(columns[0].equals("D")) {

			
			for(IntWritable value : values) {
				if(bMonth != key.getMonth()) {
					result.set(sum);
					//D, 1987
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("departure", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
				
			}
			if (bMonth == key.getMonth()) {
				result.set(sum);
				//D, 1987
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(bMonth);
				mos.write("departure", outputKey, result);
				
			}
		}else {
			for(IntWritable value : values) {
				if(bMonth != key.getMonth()) {
					result.set(sum);
					//D, 1987
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("arrival", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
				
			}
			if (bMonth == key.getMonth()) {
				result.set(sum);
				//D, 1987
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(bMonth);
				mos.write("arrival", outputKey, result);
				
			}
			
			//mos.write("arrival",  outputKey, result);
			
		}	
		
	}//--reduce

	@Override
	protected void cleanup(Context context)
			throws IOException, InterruptedException {
		mos.close();
	}
	
	
	
}

```

- SortDelayCount.java : Driver class  

```java
package com.sist;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.util.GenericOptionsParser;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.fs.Path;

public class SortDelayCount extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(),args).getRemainingArgs();
		if(otherArgs.length != 2) {
			System.out.println("Use SortDelayCount <in> <out>");
			System.exit(2);
		}
		
		Job job = new Job(getConf(), "SortDelayCount");
		
		//입출력 경로
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		//Job class
		job.setJarByClass(SortDelayCount.class);
		job.setPartitionerClass(GroupKeyPartitioner.class);
		job.setGroupingComparatorClass(GroupKeyComparator.class);
		job.setSortComparatorClass(DateKeyComparator.class);
		
		//Mapper
		job.setMapperClass(SortDelayCountMapper.class);
		
		//Reducer
		job.setReducerClass(SortDelayCountReducer.class);
		
		//Mapper key, value
		job.setMapOutputKeyClass(DateKey.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		//입출력 데이터 포맷
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		//출력 key, value
		job.setOutputKeyClass(DateKey.class);
		job.setOutputValueClass(IntWritable.class);
		
		//MultipleOutputs 설정
		MultipleOutputs.addNamedOutput(job, "departure", TextOutputFormat.class, 
																						DateKey.class, IntWritable.class);
		MultipleOutputs.addNamedOutput(job, "arrival", TextOutputFormat.class, 
				DateKey.class, IntWritable.class);

		job.waitForCompletion(true);
		
		
		return 0;
	}
	
	public static void main(String args[]) throws Exception {
		//tool inferface 실행
		int res = ToolRunner.run(new Configuration(), new SortDelayCount(), args);
		System.out.println("**res = " + res);
	}



}

```

jar 파일 실행  

```
hadoop jar SortDelayCount.jar com.sist.SortDelayCount /input delay_count_sort
```

##### 이런 온갖 클래스가 필요한 이유?  
데이터들이 node에 각각 쪼개져 있기 때문이다.  

---

### Hadoop테스트하기  
커다란 규모의 데이터를 돌리기 전에 작은 규모로 테스트해보자  
이전 WordCount 프로젝트에서 작업  

- MRUnit  
https://mvnrepository.com/artifact/org.apache.mrunit/mrunit/1.1.0  
pom.xml에 추가, dependency 아래에 classifier를 꼭 추가해 줄 것!  
```xml
<!-- Hadoop test -->
<!-- https://mvnrepository.com/artifact/org.apache.mrunit/mrunit -->
	<dependency>
	    <groupId>org.apache.mrunit</groupId>
	    <artifactId>mrunit</artifactId>
	    <version>1.0.0</version>
	    <classifier>hadoop2</classifier>
	    <scope>test</scope>
	</dependency>
		
```
- 참고 : maven classifier
http://windwolf.tistory.com/18

- WordCountTest.java  
```java
package com.sist;

import java.io.IOException;
import java.util.List;
import java.util.ArrayList;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mrunit.mapreduce.MapDriver;
import org.apache.hadoop.mrunit.mapreduce.ReduceDriver;
import org.apache.hadoop.mrunit.mapreduce.MapReduceDriver;

import org.junit.Before;
import org.junit.Test;

public class WordCountTest {
	/**
	 * input.txt
	 * ==========
	 * read a book  	read : 1 a : 1 book : 1
	 * write a book
	 * ============
	 */

	MapDriver<LongWritable, Text, Text, IntWritable> mapDriver;
	
	ReduceDriver<Text, IntWritable, Text, IntWritable> reduceDriver;
	
	MapReduceDriver<LongWritable, Text, Text, IntWritable, Text, IntWritable> mapReduceDriver;
	
	@Before
	public void setUp() {
		WordCountMapper mapper = new WordCountMapper();
		WordCountReducer reducer = new WordCountReducer();
		
		mapDriver = new MapDriver<LongWritable, Text, Text, IntWritable>();
		mapDriver.setMapper(mapper);
		
		reduceDriver = new ReduceDriver<Text, IntWritable, Text, IntWritable>();
		reduceDriver.setReducer(reducer);
		
		mapReduceDriver = new MapReduceDriver<LongWritable, Text, Text, IntWritable, Text, IntWritable>();
		mapReduceDriver.setMapper(mapper);
		mapReduceDriver.setReducer(reducer);
		
	}
	
	@Test
	public void mapperTest() throws IOException {
		mapDriver.withInput(new LongWritable(1), new Text("read a book"));
		mapDriver.withInput(new LongWritable(2), new Text("write a book"));

		mapDriver.withOutput(new Text("read"),new IntWritable(1));
		mapDriver.withOutput(new Text("a"),new IntWritable(1));
		mapDriver.withOutput(new Text("book"),new IntWritable(1));
		
		mapDriver.withOutput(new Text("write"),new IntWritable(1));
		mapDriver.withOutput(new Text("a"),new IntWritable(1));
		mapDriver.withOutput(new Text("book"),new IntWritable(1));
		
		mapDriver.runTest();
		
	}//-mapperTest

	@Test
	public void reduceTest() throws IOException {
		List<IntWritable> values = new ArrayList<IntWritable>();
		values.add(new IntWritable(1));
		values.add(new IntWritable(1));
		
		reduceDriver.withInput(new Text("book"),values);
		reduceDriver.withOutput(new Text("book"),new IntWritable(2));
		
		reduceDriver.runTest();
		
	}//-reduceTest

	@Test
	public void mapReduceTest() throws IOException {
		mapReduceDriver.withInput(new LongWritable(1), new Text("read a book"));
		mapReduceDriver.withInput(new LongWritable(2), new Text("write a book"));
		
		//순서 주의
		mapReduceDriver.addOutput(new Text("a"), new IntWritable(2));
		mapReduceDriver.addOutput(new Text("book"), new IntWritable(2));
		mapReduceDriver.addOutput(new Text("read"), new IntWritable(1));
		mapReduceDriver.addOutput(new Text("write"), new IntWritable(1));

		mapReduceDriver.runTest();
	}
	
}

```


- mapperTest의 오류  
```
java.lang.AssertionError: 1 Error(s): (Received unexpected output (book, 1) at position 5.)
```

- reduceTest의 오류  
```
java.lang.AssertionError: 2 Error(s): (Missing expected output (book, 3) at position 0., Received unexpected output (book, 2) at position 0.)
```

- mapReduceTest의 순서 오류
```
java.lang.AssertionError: 2 Error(s): (Matched expected output (write, 1) but at incorrect position 3 (expected position 2), Matched expected output (read, 1) but at incorrect position 2 (expected position 3))
```

