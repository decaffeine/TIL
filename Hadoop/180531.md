# 180531 

### 네임노드와 데이터노드  
HDFS는 master-slave architecture  
```
master : NameNode  
slave : DataNode  
```
[그림]

### 보조네임노드   
NameNode는 메타데이터를 메모리에서 처리한다.  
이 메타데이터의 유실 방지를 위해 editslog와 fsimage라는 파일을 생성  
(editslog : HDFS의 모든 변경 이력 저장, fsimage : 메모리에 저장된 메타데이터의 파일 시스템 이미지 저장)  
보조네임노드를 통해 주기적으로 네임노드의 fsimage를 갱신 (checkpoint 작업)  
checkpoint 작업은 기본적으로 1시간마다 한 번씩 일어남

### 분석용 데이터  

http://stat-computing.org/dataexpo/2009/  
1987년 다운로드  
홈 디렉토리의 dataexpo 폴더에 옮기기  

### HDFS 명령어  

hdfs상의 모든 파일과 디렉토리 표시
```
hdfs dfs -ls -R /
```

/에 input이라는 디렉토리 만들기 
```
hdfs dfs -mkdir /input
```

현재 폴더의 1987.csv를 hdfs상의 가상 폴더인 input으로 올리기
```
hdfs dfs -put ./1987.csv /input
```

hdfs상의 input/1987.csv를 실제 폴더로 내리기
```
hdfs dfs -get /input/1987.csv
```

- Linux 명령어 cat, tail  
```
cat : 파일의 내용 출력
tail : 파일의 끝부분 출력
```

hadoop에서도 똑같이 쓸 수 있다.  

```
hdfs dfs -tail /input/1987.csv 
hdfs dfs -cat /input/1987.csv | tail -10
hdfs dfs -cat /input/1987.csv | head -10

```

파일 삭제하기  
```
hdfs dfs -rm /input/1987.csv
hdfs dfs -rm -R /input/1987.csv
```
-R 옵션 : 폴더까지 같이 지울 때  

#### 새 프로젝트 : big_01_HDFS_IN_OUT

```
New Maven Project
maven-quickstart
Group Id : com.sist
Artifact Id : big_01_HDFS_IN_OUT

```

기존 pom.xml 가져오기  

- 라이브러리 추가  
```xml

 	<properties>
 			<org.springframework.data.version>2.4.0.RELEASE</org.springframework.data.version>
 			<org.apache.hadoop.version>2.8.0</org.apache.hadoop.version>
	</properties>

	<dependencies>
	
	<!--  hadoop-spring-data -->
	<!-- https://mvnrepository.com/artifact/org.springframework.data/spring-data-hadoop -->
<dependency>
    <groupId>org.springframework.data</groupId>
    <artifactId>spring-data-hadoop</artifactId>
    <version>${org.springframework.data.version}</version>
</dependency>
	
	<!--  Hadoop Client -->
	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>${org.apache.hadoop.version}</version>
</dependency>

	</dependencies>

```

https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client/2.8.0  
https://mvnrepository.com/artifact/org.springframework.data/spring-data-hadoop/2.4.0.RELEASE  

#### 작업 순서  
0. (Hadoop 실행중)  
1. Java Coding  
2. Jar로 압축  
3. NameNode에서 실행  


#### SingleFileWriteRead.java  

```java
package com.sist;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * Java <-> Hadoop File In/Out
 * @author sist
 *
 */
public class SingleFileWriteRead {

	public static void main(String[] args) {
		// 입력 파라미터
		if(args.length != 2) {
			System.err.println("SingleFileWriteRead <filename> <contents>");
			System.exit(2);
		}
		
		try {
			//파일 시스템 제어 객체
			Configuration conf = new Configuration();
			FileSystem hdfs = FileSystem.get(conf);
			
			//filename
			Path path = new Path(args[0]);
			if(hdfs.exists(path)) {
				hdfs.delete(path, true);
			}
			
			//파일 저장
			FSDataOutputStream outStream = hdfs.create(path);
			outStream.writeUTF(args[1]);
			outStream.close();
			
			//파일 출력
			FSDataInputStream inputStream = hdfs.open(path);
			String inputString = inputStream.readUTF();
			inputStream.close();
			
			System.out.println("\n inputString = " + inputString);
			
		}catch(Exception e) {
			e.printStackTrace();
		}//--try-catch
		

	}//--main

}
```

jar 파일로 export하자. (미리 만들어 놓은 hadoop_user_jar 폴더에)  
```
export - JAR file - hadoop_user_jar/hadoop_ex01.jar
```

```
cd ~
cd hadoop_user_jar
hadoop jar hadoop_ex01.jar com.sist.SingleFileWriteRead input.txt HelloHadoop
```

실행 결과  
```
inputString = HelloHadoop
```

hdfs상의 파일들을 보면 input.txt가 만들어져 있는 것을 확인할 수 있다.  
```
hdfs dfs -ls -R /
```

우리가 만든 input.txt의 내용을 보자  
```
hdfs dfs -cat /user/sist/input.txt  
```

#### maven - build  
pom.xml  
```xml
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>1.2.1</version>
                <configuration>
                    <mainClass>com.sist.SingleFileWriteRead</mainClass>
                </configuration>
            </plugin>
```
  
Run As - Maven Build  
Goals : package  
target 폴더에 생긴 jar 파일 확인  

### 새 프로젝트 : WordCount
```
maven project
maven-quickstart
Groud Id : com.sist
Artifact Id : WordCount
Package : com.sist
```

---  
  
WordCountMapper.java  
WordCountReducer.java  
WordCount.java : 실행 순서 


|              | WordCountMapper          | WordCountReducer                              | 출력                          |
|--------------|--------------------------|-----------------------------------------------|-------------------------------|
| read a book  | read : 1 a : 1 book : 1  | read : {1} a : {1,1} book : {1,1} write : {1} | read  1 a  2 book  2 write  1 |
| write a book | write : 1 a : 1 book : 1 |                                               |                               |


#### WordCountMapper
```java
package com.sist;

import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;

/**
 * input.txt
 * =============
 * read a book				read : 1 a : 1 book : 1
 * write a book
 * =============
 * @author sist
 *
 */
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

	private final IntWritable ONE = new IntWritable(1);
	private Text word = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
			
		StringTokenizer st = new StringTokenizer(value.toString());
		
		while(st.hasMoreTokens()) {
			word.set(st.nextToken());
			context.write(word, ONE);
		}//--while
			
	}//--map
	

}

```


#### WordCountReducer

```java
package com.sist;

import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.io.Text;
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
/**
 * read:1 a:1 book:1 -> read 1, a 2
 * write:1 a:1 book:1 / 사실상 a:{1,1} book{1,1}
 * @author sist
 *
 */
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
	private IntWritable result = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable>  values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		int sum = 0;
		
		for(IntWritable val:values) {
			sum += val.get();
		}
		result.set(sum);
		context.write(key, result);
	}
	
	

}

```

#### WordCount

```java
package com.sist;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

/**
 * 실행순서
 * 
 * 1. Job 객체 생성
 * 2. Job 객체에 map, reducer 정보를 설정
 * 3. Job 실행
 * 
 * In/Out DataFormat
 * @author sist
 *최종 /output/part-00000
 */
public class WordCount {
	
	public static void main(String[] args) 
			throws IOException, ClassNotFoundException, InterruptedException {
		
		Configuration conf = new Configuration();
		
		// 1. Job 객체 생성
		Job job = new Job(conf, "WordCount");
		
		//2. Job 객체에 map, reducer 정보를 설정  
		job.setJarByClass(WordCount.class);
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);
		//dataformat
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		//2.1. 입출력 경로 지정
		FileInputFormat.addInputPath(job, new Path("/input"));
		FileOutputFormat.setOutputPath(job, new Path("/output"));
		
		// 3. Job 실행
		job.waitForCompletion(true);
		
	}//--main

}

```

jar 파일로 export하고 hadoop_user_jar로 옮기기  

---  

하둡 디렉토리에서 유저 디렉토리 모두 삭제  
```
hdfs dfs -rm -r /user
```

input.txt에 input 내용 작성
```
gedit input.txt
read a book
write a book
```

hdfs상의 input 폴더에 input.txt 올림  
```
hdfs dfs -put ./input.txt /input
```
  
WordCount.jar 있는지 확인  
```
cd hadoop_user_jar
ls WordCount.jar
```

WordCount.jar 실행  
```
hadoop jar WordCount.jar com.sist.WordCount
```

실행한 것 확인  
```
hdfs dfs -cat /output/part-r-00000
```

방금 작업한 것 확인  
```
web > http://localhost:8088/cluster
```
