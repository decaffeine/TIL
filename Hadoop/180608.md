# 180608 

### Join at MapReduce

- data : carriers.csv  
http://stat-computing.org/dataexpo/2009/supplemental-data.html  
첫 줄과 "들 제거  

### 새 프로젝트 : JoinDelayCount  

- MapSideJoin
  - Codes
    - AirlinePerformanceParser.java  
    - CarrierCodeParser.java  
    - MapperWithMapSideJoin.java  
    - MapSideJoin.java  
  - Data  
    - carriers.csv  
    - 2008.csv  
  
carriers.csv는 /meta에 올리기  
```
hdfs dfs -mkdir /meta
hdfs dfs ~/dataexpo/carriers.csv /meta
```

- CarrierCodeParser.java  
```java
package com.sist;

import org.apache.hadoop.io.Text;

public class CarrierCodeParser {
	private String carrierCode;
	private String carrierName;
	
	public CarrierCodeParser(Text value) {
		this(value.toString());
	}
	
	public CarrierCodeParser(String value) {
		try {
			String[] columns = value.split(",");
			if(null != columns && columns.length > 0) {
				carrierCode = columns[0];
				carrierName = columns[1];
			}
			
		}catch(Exception ex) {
			System.out.println("======");
			System.out.println(ex.getMessage());
			System.out.println("======");
		}//--try-catch
	}

	public String getCarrierCode() {
		return carrierCode;
	}

	public void setCarrierCode(String carrierCode) {
		this.carrierCode = carrierCode;
	}

	public String getCarrierName() {
		return carrierName;
	}

	public void setCarrierName(String carrierName) {
		this.carrierName = carrierName;
	}
	
	

}

```

- MapperWithMapSideJoin.java  
```java
package com.sist;

import org.apache.hadoop.mapreduce.Mapper;

import com.thoughtworks.xstream.io.binary.Token.Value;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.filecache.DistributedCache;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;

import java.util.Hashtable;

public class MapperWithMapSideJoin extends Mapper<LongWritable, Text, Text, Text> {
	
	private Hashtable<String, String> joinMap = new Hashtable<String, String>();
	
	private Text outputKey = new Text();

	@Override
	protected void setup(Mapper<LongWritable, Text, Text, Text>.Context context)
			throws IOException, InterruptedException {
		
		try {
			Path[] cacheFiles = DistributedCache.getLocalCacheFiles(context.getConfiguration());
			
			//데이터 생성
			
			if(null != cacheFiles && cacheFiles.length > 0 ) {
				String line ;
				BufferedReader br = new BufferedReader(new FileReader(cacheFiles[0].toString()));
				try {
					while((line = br.readLine())!= null) {
						CarrierCodeParser codeParser = new CarrierCodeParser(line);
						joinMap.put(codeParser.getCarrierCode(), codeParser.getCarrierName());
					}//--while
				} finally {
					br.close();
				}//--try-finally
				
			} else {
				System.out.println("**cache file is null");
			}//--if-else
			
		} catch (Exception e) {
			System.out.println("======");
			System.out.println(e.getMessage());
			System.out.println("======");
			
		}//--try-catch
		
	}//--map

	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context)
			throws IOException, InterruptedException {
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		this.outputKey.set(parser.getUniqueCarrier());
		
		context.write(outputKey, new Text(joinMap.get(parser.getUniqueCarrier())+"\t"+value.toString()));
	}//--map
	
	
	
	

}

```

- MapSideJoin.java : Driver class  
```java
package com.sist;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.util.GenericOptionsParser;




public class MapSideJoin extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		//<metadata> <in> <out>
		
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		if(otherArgs.length != 3) {
			System.out.println("Use : <metadata> <in> <out>");
			System.exit(2);
		}
		
		Job job = new Job(getConf(),"MapSideJoin");
		
		//분산 캐시
		DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
		
		//output 존재하면 지우기
		Configuration conf = new Configuration();
		FileSystem hdfs = FileSystem.get(conf);
		Path path = new Path(otherArgs[2]);
		
		if(hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		FileInputFormat.addInputPath(job, new Path(otherArgs[1]));		
		FileOutputFormat.setOutputPath(job, path);
		
		//Job class
		job.setJarByClass(MapSideJoin.class);
		
		//Mapper
		job.setMapperClass(MapperWithMapSideJoin.class);
		
		//Reducer (없음)
		job.setNumReduceTasks(0);
		
		//입출력 데이터 포맷
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		//출력키, 출력 값
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		job.waitForCompletion(true);
		
		return 0;
	}//--run

	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new MapSideJoin(), args);
		System.out.println("**res=" + res);
	}//--main

}

```

- 실행
```
hadoop jar JoinDelayCount.jar com.sist.MapSideJoin /meta/carriers.csv /input/2008.csv map_join
```

- 결과 보기  
```
hdfs dfs -text /user/sist/map_join/part-m-00000 | head -10
```

- 결과  
```
WN	Southwest Airlines Co.	2008,1,3,4,2003,1955,2211,2225,WN,335,N712SW,128,150,116,-14,8,IAD,TPA,810,4,8,0,,0,NA,NA,NA,NA,NA
...
OO	Skywest Airlines Inc.	2008,3,16,7,554,600,747,736,OO,6517,N713SK,113,96,71,11,-6,TUS,LAX,451,30,12,0,,0,NA,NA,NA,NA,NA


```

---

- ReduceSideJoin  
  - Codes  
    - AirlinePerformanceParser.java  
    - CarrierCodeParser.java  

    - CarrierCodeMapper.java
    - MapperWithReducerSideJoin.java  
    - ReducerSideJoin.java  
  
    - TaggedKey.java  
    - TaggedKeyComparator.java  
    - TaggedGroupKeyPartitioner.java  
    - TaggedGroupKeyComparator.java  
  - Data  
    - carriers.csv  
    - 2008.csv  


- TaggedKey.java
```java
package com.sist.reducejoin;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableUtils;

public class TaggedKey implements WritableComparable<TaggedKey> {
	
	//항공사 코드
	private String carrierCode;
	
	//join tag : Alias
	private Integer tag;
	
	public String getCarrierCode() {
		return carrierCode;
	}

	public Integer getTag() {
		return tag;
	}
	
	
	public void setCarrierCode(String carrierCode) {
		this.carrierCode = carrierCode;
	}

	public void setTag(Integer tag) {
		this.tag = tag;
	}

	public TaggedKey() {}

	public TaggedKey(String carrierCode, Integer tag) {
		this.carrierCode = carrierCode;
		this.tag = tag;
	}

	@Override
	public void readFields(DataInput in) throws IOException {
		this.carrierCode = WritableUtils.readString(in);
		tag = in.readInt();
	}

	@Override
	public void write(DataOutput out) throws IOException {
		WritableUtils.writeString(out, this.carrierCode);
		out.writeInt(tag);
	}

	@Override
	public int compareTo(TaggedKey key) {
		int result = this.carrierCode.compareTo(key.carrierCode);
		if(result == 0) {
			return this.tag.compareTo(key.tag);
		}
		return result;
	}
	

}

```


- TaggedKeyComparator.java
```java
package com.sist.reducejoin;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class TaggedKeyComparator extends WritableComparator {
	protected TaggedKeyComparator() {
		super(TaggedKey.class,true);
	}

	@Override
	public int compare(WritableComparable w1, WritableComparable w2) {
			
		TaggedKey k1 = (TaggedKey) w1;
		TaggedKey k2 = (TaggedKey) w2;
		
		int cmp = k1.getCarrierCode().compareTo(k2.getCarrierCode());
		if(cmp != 0) {
			return cmp;
		}
		return k1.getTag().compareTo(k2.getTag());
	}

}

```


- TaggedGroupKeyPartitioner.java  
```java
package com.sist.reducejoin;

import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.io.Text;

public class TaggedGroupKeyPartitioner extends Partitioner<TaggedKey, Text> {

	@Override
	public int getPartition(TaggedKey key, Text val, int numPartitions) {
		int hash = key.getCarrierCode().hashCode();
		int partition = hash % numPartitions;
		return partition;
	}
}


```

- TaggedGroupKeyComparator.java
```java
package com.sist.reducejoin;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class TaggedGroupKeyComparator extends WritableComparator {
	
	protected TaggedGroupKeyComparator() {
		super(TaggedKey.class, true);
	}

	@Override
	public int compare(WritableComparable w1, WritableComparable w2) {
		TaggedKey k1 = (TaggedKey) w1;
		TaggedKey k2 = (TaggedKey) w2;
		
		return k1.getCarrierCode().compareTo(k2.getCarrierCode());
	}

}

```


- CarrierCodeMapper.java
```java
package com.sist.reducejoin;

import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import com.sist.CarrierCodeParser;

public class CarrierCodeMapper extends Mapper<LongWritable, Text, TaggedKey, Text> {
	
	TaggedKey outputKey = new TaggedKey();
	Text outValue = new Text();
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, TaggedKey, Text>.Context context)
			throws IOException, InterruptedException {
		CarrierCodeParser parser = new CarrierCodeParser(value);
		outputKey.setCarrierCode(parser.getCarrierCode());
		outputKey.setTag(0);
		
		outValue.set(parser.getCarrierName());
		
		context.write(outputKey, outValue);
	}
	
	

}

```


- MapperWithReduceSideJoin.java
```java
package com.sist.reducejoin;

import org.apache.hadoop.mapreduce.Mapper;

import com.sist.AirlinePerformanceParser;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;


public class MapperWithReduceSideJoin extends Mapper<LongWritable, Text, TaggedKey, Text> {

	TaggedKey outputKey = new TaggedKey();
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, TaggedKey, Text>.Context context)
			throws IOException, InterruptedException {
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		outputKey.setCarrierCode(parser.getUniqueCarrier());
		outputKey.setTag(1);
		context.write(outputKey, value);
	}

	
}

```

- 책 231페이지 참고  

- ReducerWithReduceSideJoin.java  
```java
package com.sist.reducejoin;

import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.Iterator;

import org.apache.hadoop.io.Text;

public class ReducerWithReduceSideJoin extends Reducer<TaggedKey, Text, Text, Text> {
	
	private Text outputKey = new Text();
	private Text outputValue = new Text();
	@Override
	protected void reduce(TaggedKey key, Iterable<Text> values, Context context)
			throws IOException, InterruptedException {
		/**
		 * <UA,0><United AirLines>
		 * <UA,1><1997,11,.,....,>
 		 * <UA,1><1997,11,.,....,>
		 * ...
		 * 
		 */
		
		Iterator<Text> iterator = values.iterator();
		
		//항공사 이름
		Text carrierName = new Text(iterator.next());
		
		//항공지연 데이터 처리
		while(iterator.hasNext()) {
			Text record = iterator.next();
			outputKey.set(key.getCarrierCode());
			outputValue = new Text(carrierName.toString() + "\t" + record.toString());
			context.write(outputKey, outputValue);
		}
		
		
	}
	
	

}

```


- ReducerSideJoin.java : Driver Class  
```java
package com.sist.reducejoin;


import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.sist.MapSideJoin;

import org.apache.hadoop.util.GenericOptionsParser;

public class ReducerSideJoin extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
	//<metadata> <in> <out>
		
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		if(otherArgs.length != 3) {
			System.out.println("Use : <metadata> <in> <out>");
			System.exit(2);
		}
		
		Job job = new Job(getConf(),"ReducerSideJoin");
		
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));
		
		//job class
		job.setJarByClass(ReducerSideJoin.class);
		job.setPartitionerClass(TaggedGroupKeyPartitioner.class);
		job.setGroupingComparatorClass(TaggedGroupKeyComparator.class);
		job.setSortComparatorClass(TaggedKeyComparator.class);
		
		//reduce
		job.setReducerClass(ReducerWithReduceSideJoin.class);
		
		job.setMapOutputKeyClass(TaggedKey.class);
		job.setMapOutputValueClass(Text.class);
		
		//입출력 format
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		//출력 key. value
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		//multiple input
		MultipleInputs.addInputPath(job, new Path(otherArgs[0]), TextInputFormat.class, CarrierCodeMapper.class);
		MultipleInputs.addInputPath(job, new Path(otherArgs[1]), TextInputFormat.class, MapperWithReduceSideJoin.class);
		
		job.waitForCompletion(true);
		return 0;
	}

	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new ReducerSideJoin(), args);
		System.out.println("**res=" + res);

	}

}


```

- 실행
```
hadoop jar JoinDelayCount.jar com.sist.reducejoin.ReducerSideJoin /meta/carriers.csv reduce_join
```
