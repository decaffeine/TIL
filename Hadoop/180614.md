# 180614 

src\main\resources
Delete log4j.xml 

---

- 프로젝트 : TwitterA  
쌓은 log를 분석해보자.

- TwitterMapper.java 

- TwitterReducer.java

- TwitterMain.java  


 
### Clustering  

한 곳으로 묶기  
싸게 할 수 있다.  

211.238.142.31	NameNode (1명)		 sist31
211.238.142.32	SecondaryNode (1명)  sist32
211.238.142.35	DataNode35 (4명)	 sist35
211.238.142.xx	DateNodexx		 sistxx
..

NameNode : 작업 지시  
SecondaryNode : 작업 부하 줄임  
DataNode : 실제 작업  

#### 할 일 목록  

0. SSH 설치  
sudo apt-get update && sudo apt-get upgrade  
sudo apt-get install openssh-server  


- Hadoop Backup하기  
/backup 디렉토리에 현재 Hadoop 폴더 파일 백업하고 파일 내용 확인  
```
cd
mkdir backup
cd backup
tar cvfz Hadoop_0614.tar.gz /usr/local/hadoop-2.7.5
tar tvf Hadoop_0614.tar.gz
```
1. System 설정  

- host명 변경  
```
hostnamectl set-hostname sist101
hostname
```

- 파일 변경
```
sudo gedit /etc/hosts


211.238.142.101	sist101	ns101	NameNode
211.238.142.106	sist106	ns106	SecondaryNode
211.238.142.111	sist111	ns111	DataNode111
211.238.142.105	sist105	ns105	DataNode105
211.238.142.110	sist110	ns110	DataNode110
211.238.142.116	sist116	ns116	DateNode116
```

- ssh 설정  
이미 ssh 디렉토리가 있다면 지우고 새로 키 생성  
```
rm -rf .ssh/
ssh-keygen -t rsa
```

```
drwx------  2 sist sist 4096  6월 14 11:11 ./
drwxr-xr-x 56 sist sist 4096  6월 14 11:11 ../
-rw-------  1 sist sist 1679  6월 14 11:11 id_rsa
-rw-r--r--  1 sist sist  394  6월 14 11:11 id_rsa.pub
```



제대로 만들어졌는지 확인 
```
cd ~/.ssh
ll
```

```
cat ./id_rsa.pub >> authorized_keys
```

(NameNode만 작업)  
```
scp ./authorized_keys sist@sist106:~/.ssh
scp ./authorized_keys sist@sist111:~/.ssh
scp ./authorized_keys sist@sist105:~/.ssh
scp ./authorized_keys sist@sist110:~/.ssh
scp ./authorized_keys sist@sist116:~/.ssh
```

비밀번호 없이 접근되는지 확인
```
ssh sist(이름) hostname
```

(다같이 작업)  

- 디렉토리 만들기  
```
sudo mkdir -p /home/hadoop/hdfs
sudo mkdir data temp name mapred
```

- 권한 sist에게 주기  
```
sudo chown -R sist:sist /home/hadoop
```

2. Hadoop 설정  

```
cd /usr/local/hadoop/etc/hadoop

```

- core-site.xml
```xml
<configuration>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/hdfs/temp</value>
	</property>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://NameNode:9000</value>	
	</property>
</configuration>
```

- hdfs-site.xml  
```xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/home/hadoop/hdfs/name</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/home/hadoop/hdfs/data</value>
	</property>
</configuration>
```

- mapred-site.xml
```xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>

	<property>
		<name>mapred.local.dir</name>
		<value>/home/hadoop/hdfs/mapred</value>
	</property>

	<property>
		<name>mapred.system.dir</name>
		<value>/home/hadoop/hdfs/mapred</value>
	</property>
</configuration>
```

- yarn-site.xml 
```xml
<configuration>

<!-- Site specific YARN configuration properties -->
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>NameNode</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>NameNode:8025</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>NameNode:8030</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>NameNode:8040</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>NameNode:8088</value>	
	</property>
</configuration>

```

- etc/environment
```xml
PATH=".:/usr/local/hive/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/jdk1.8.0_171/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games"

JAVA_HOME="/usr/local/jdk_1.8.0_171"
HIVE_HOME="/usr/local/hive"
CLASSPATH=$JAVA_HOME/lib:home/sist/hadoop_user_jar

HADOOP_HOME="/usr/local/hadoop"
HADOOP_MAPRED_HOME="/usr/local/hadoop"
HADOOP_COMMON_HOME="/usr/local/hadoop"
HADOOP_HDFS_HOME="/usr/local/hadoop"
HADOOP_COMMON_LIB_NATIVE_DIR="/usr/local/hadoop/lib/native"
HADOOP_OPTS="-Djava.library.path=/usr/local/hadoop/lib"
YARN_HOME="/usr/local/hadoop"
HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
YARN_CONF_DIR="/usr/local/hadoop/etc/hadoop"
```

- yarn-env.sh  
```
export JAVA_HOME="/usr/local/jdk_1.8.0_171"
export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_MAPRED_HOME="/usr/local/hadoop"
export HADOOP_COMMON_HOME="/usr/local/hadoop"
export HADOOP_HDFS_HOME="/usr/local/hadoop"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
export YARN_HOME="/usr/local/hadoop"
export YARN_CONF_DIR="/usr/local/hadoop/etc/hadoop"
```

- 찌꺼기 지우기  

- usr/local/hadoop/etc/hadoop/slaves (파일)
```
DataNode111
DataNode105
DataNode110
DataNode116
```

- usr/local/hadoop/etc/hadoop/masters (파일)
```
SecondaryNode
```

3. 통신 설정  


##### Hadoop Backup  

