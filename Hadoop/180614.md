# 180614 

- src\main\resources  
log4j.xml 삭제하기  
---

- 프로젝트 : TwitterA  
쌓은 log를 분석해보자.  

- TwitterMapper.java 
```java
package com.sist.anal;

import org.apache.hadoop.mapreduce.Mapper;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.IntWritable;

import java.io.IOException;
import java.util.regex.*;



public class TwitterMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
	
	private final IntWritable one = new IntWritable(1);
	
	private Text result = new Text();
	
	String[] data = {"SBS","MBC","jtbc"};

	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		
		Pattern[] p = new Pattern[data.length];
		
		for(int i = 0 ; i < p.length ; i++) {
			p[i] = Pattern.compile(data[i]);
		}
		
		Matcher[] m = new Matcher[data.length];
		
		for(int i = 0 ; i < m.length ; i++) {
			m[i] = p[i].matcher(value.toString());
			
			if(m[i].find() == true) {
				result.set(data[i]);
				context.write(result,  one);
			}
		}
		
	}//--map
	
	

}

```
- TwitterReducer.java  
```java
package com.sist.anal;

import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.IntWritable;

import java.io.IOException;



public class TwitterReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

	private IntWritable res = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		
		int sum = 0;
		for(IntWritable i : values) {
			sum += i.get();
		}
		
		res.set(sum);
		context.write(key, res);
	}
	
	
	
}


```


- TwitterMain.java  
```java
package com.sist.anal;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileSystem;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class TwitterMain {

	public static void main(String[] args) {
		try {
			Configuration conf=new Configuration();
			Job job=new Job(conf,"TwitterMain"); 
			
			
			//Output삭제
			FileSystem hdfs = FileSystem.get(conf);
			
			//filename
			Path outPath =new Path("./output");
			if(hdfs.exists(outPath)) {
				hdfs.delete(outPath, true);
			}
			
			
			job.setMapperClass(TwitterMapper.class);
			job.setReducerClass(TwitterReducer.class);
			job.setJarByClass(TwitterMain.class);
			
			job.setOutputKeyClass(Text.class);
			job.setOutputValueClass(IntWritable.class);
		
			
			FileInputFormat.addInputPath(job, new Path("./input/app1.log"));
			FileOutputFormat.setOutputPath(job, outPath);
			
			job.waitForCompletion(true);
			
			System.out.println("========================");
			System.out.println("완료");
			System.out.println("========================");
		}catch(Exception e) {
			System.out.println("========================");
			e.printStackTrace();
			System.out.println("========================");
		}
		

	}

}

```

output/part-r-0000 파일을 보자  
hdfs를 실행하지 않고도 output을 볼 수 있다!  

---
 
### Clustering  

지금까지는 가상 모드로 한 컴퓨터에서만 Hadoop을 돌렸지만, 
실제 환경에서는 여러 대의 컴퓨터를 사용하여 한 곳으로 묶는 Clustering을 사용한다.  
비용이 저렴  

211.238.142.31	NameNode (1명)		 sist31
211.238.142.32	SecondaryNode (1명)	 sist32
211.238.142.35	DataNode35 (4명)	 sist35
211.238.142.xx	DateNodexx		 sistxx
..

NameNode : 작업 지시  
SecondaryNode : 작업 부하 줄임  
DataNode : 실제 작업  

#### 할 일 목록  

0. SSH 설치  
```
sudo apt-get update && sudo apt-get upgrade  
sudo apt-get install openssh-server  
```

- Hadoop Backup하기  
/backup 디렉토리에 현재 Hadoop 폴더 파일 백업하고 파일 내용 확인  
```
cd
mkdir backup
cd backup
tar cvfz Hadoop_0614.tar.gz /usr/local/hadoop-2.7.5
tar tvf Hadoop_0614.tar.gz
```
1. System 설정  

- host명 변경  
```
hostnamectl set-hostname sist101
hostname
```

- /etc/hosts 파일 변경  
```
sudo gedit /etc/hosts


211.238.142.101	sist101	ns101	NameNode
211.238.142.106	sist106	ns106	SecondaryNode
211.238.142.111	sist111	ns111	DataNode111
211.238.142.105	sist105	ns105	DataNode105
211.238.142.110	sist110	ns110	DataNode110
211.238.142.116	sist116	ns116	DateNode116
```

- ssh 설정  
이미 ssh 디렉토리가 있다면 지우고 새로 키 생성  
```
rm -rf .ssh/
ssh-keygen -t rsa
```

  - 생성 결과  
```
drwx------  2 sist sist 4096  6월 14 11:11 ./
drwxr-xr-x 56 sist sist 4096  6월 14 11:11 ../
-rw-------  1 sist sist 1679  6월 14 11:11 id_rsa
-rw-r--r--  1 sist sist  394  6월 14 11:11 id_rsa.pub
```


  - 제대로 만들어졌는지 확인   
```
cd ~/.ssh
ll
```
  
```
cat ./id_rsa.pub >> authorized_keys
```

- ssh key 복사하기 (여기는 NameNode만 작업)  
```
scp ./authorized_keys sist@sist106:~/.ssh
scp ./authorized_keys sist@sist111:~/.ssh
scp ./authorized_keys sist@sist105:~/.ssh
scp ./authorized_keys sist@sist110:~/.ssh
scp ./authorized_keys sist@sist116:~/.ssh
```

  - 비밀번호 없이 접근되는지 확인  
```
ssh sist(이름) hostname
```

- 물리 디렉토리 만들기 : 여기는 다같이 작업  

```
sudo mkdir -p /home/hadoop/hdfs
sudo mkdir data temp name mapred
```

  - 권한 sist에게 주기  
```
sudo chown -R sist:sist /home/hadoop
```

2. Hadoop 설정  

```
cd /usr/local/hadoop/etc/hadoop
```

- core-site.xml
```xml
<configuration>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/hdfs/temp</value>
	</property>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://NameNode:9000</value>	
	</property>
</configuration>
```

- hdfs-site.xml  
```xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/home/hadoop/hdfs/name</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/home/hadoop/hdfs/data</value>
	</property>
</configuration>
```

- mapred-site.xml
```xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>

	<property>
		<name>mapred.local.dir</name>
		<value>/home/hadoop/hdfs/mapred</value>
	</property>

	<property>
		<name>mapred.system.dir</name>
		<value>/home/hadoop/hdfs/mapred</value>
	</property>
</configuration>
```

- yarn-site.xml 
```xml
<configuration>

<!-- Site specific YARN configuration properties -->
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>NameNode</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>NameNode:8025</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>NameNode:8030</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>NameNode:8040</value>	
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>NameNode:8088</value>	
	</property>
</configuration>

```

- etc/environment
```xml
PATH=".:/usr/local/hive/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/jdk1.8.0_171/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games"

JAVA_HOME="/usr/local/jdk1.8.0_171"
HIVE_HOME="/usr/local/hive"
CLASSPATH=$JAVA_HOME/lib:home/sist/hadoop_user_jar

HADOOP_HOME="/usr/local/hadoop"
HADOOP_MAPRED_HOME="/usr/local/hadoop"
HADOOP_COMMON_HOME="/usr/local/hadoop"
HADOOP_HDFS_HOME="/usr/local/hadoop"
HADOOP_COMMON_LIB_NATIVE_DIR="/usr/local/hadoop/lib/native"
HADOOP_OPTS="-Djava.library.path=/usr/local/hadoop/lib"
YARN_HOME="/usr/local/hadoop"
HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
YARN_CONF_DIR="/usr/local/hadoop/etc/hadoop"
```

- yarn-env.sh  
```
export JAVA_HOME="/usr/local/jdk1.8.0_171"
export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_MAPRED_HOME="/usr/local/hadoop"
export HADOOP_COMMON_HOME="/usr/local/hadoop"
export HADOOP_HDFS_HOME="/usr/local/hadoop"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
export YARN_HOME="/usr/local/hadoop"
export YARN_CONF_DIR="/usr/local/hadoop/etc/hadoop"
```

- 찌꺼기 지우기  

- usr/local/hadoop/etc/hadoop/slaves (파일)
```
DataNode111
DataNode105
DataNode110
DataNode116
```

- usr/local/hadoop/etc/hadoop/masters (파일)
```
SecondaryNode
```


- 참고 : Namenode에서 hdfs를 한 번 올렸다가 다시 올릴 때는 폴더 삭제하고 다시 만듬
```
sist@sist101:/home/hadoop/hdfs$ rm -rf data mapred name temp
sist@sist101:/home/hadoop/hdfs$ sudo mkdir data mapred name temp
```

- 드디어 NameNode에서 작업을 올려보자 (예제 : SortDelayCount)   

DataNode들을  
http://211.238.142.101:8088  
http://localhost:50070/  
에서 확인할 수 있다.  

![screenshot03](https://user-images.githubusercontent.com/33911258/41402095-670e45f8-6ffc-11e8-935a-e71f1f293aa4.png)  

![screenshot02](https://user-images.githubusercontent.com/33911258/41402097-67d3d93a-6ffc-11e8-859a-533d6047b449.png)  

![screenshot01](https://user-images.githubusercontent.com/33911258/41402099-67fea660-6ffc-11e8-9d6d-2eef07b4bd12.png)
