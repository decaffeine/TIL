# 180607

#### 새 프로젝트 : SequenceFileCreator  
```
SequenceFileCreator : Driver Class
DistanceMapper : Inner Class
Reducer : 없음
```

- SequenceFileCreator.java  

```java
package com.sist;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.SequenceFileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/**
 * Driving Class
 * Map File : Inner Class
 * @author sist
 *
 */
public class SequenceFileCreator extends Configured implements Tool {

	static class DistanceMapper extends MapReduceBase implements Mapper<LongWritable, Text, IntWritable, Text> {

		private IntWritable outputKey = new IntWritable();
		
		@Override
		public void map(LongWritable key, Text value, OutputCollector<IntWritable, Text> output, Reporter reporter)
				throws IOException {
			// key : 항공 운항 거리
			AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
			try {
				if(parser.isDistanceAvailable()) {
					outputKey.set(parser.getDistance());
					output.collect(outputKey, value);
				}
			} catch (ArrayIndexOutOfBoundsException ae) {
				outputKey.set(0);
				output.collect(outputKey, value);
				ae.printStackTrace();
			} catch (Exception e) {
				outputKey.set(0);
				output.collect(outputKey, value);
				e.printStackTrace();
			}//--try-catch
			
		}
		
	}//--DistanceMapper
	
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new SequenceFileCreator(), args);
		System.out.println("***res = " + res);

	}//--main

	@Override
	public int run(String[] args) throws Exception {
		JobConf conf = new JobConf(SequenceFileCreator.class);

		conf.setJobName("SequenceFileCreator");
		conf.setMapperClass(DistanceMapper.class);
		//reducer 없음 -> ReduceTask를 0으로 설정
		conf.setNumReduceTasks(0);
		
		Configuration config = new Configuration();
		FileSystem hdfs = FileSystem.get(config);
		
		Path path = new Path(args[1]);
		if(hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		//입출력 경로
		FileInputFormat.setInputPaths(conf, new Path(args[0]));
		FileOutputFormat.setOutputPath(conf, path);
		
		//출력 포맷
		conf.setOutputFormat(SequenceFileOutputFormat.class);
		
		//출력 key
		conf.setOutputKeyClass(IntWritable.class);
		//출력 value
		conf.setOutputValueClass(Text.class);
		
		//SequenceFile 압축 Format 설정
		SequenceFileOutputFormat.setCompressOutput(conf, true);
		SequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);
		SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);
		
		JobClient.runJob(conf);
		return 0;
	}//--run

}

```

```
hdfs dfs -text /user/sist/2008_sequencefile/part-00000 | head -10
```

```
18/06/07 10:05:21 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/06/07 10:05:21 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 10:05:21 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 10:05:21 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 10:05:21 INFO compress.CodecPool: Got brand-new decompressor [.gz]
810	2008,1,3,4,2003,1955,2211,2225,WN,335,N712SW,128,150,116,-14,8,IAD,TPA,810,4,8,0,,0,NA,NA,NA,NA,NA
810	2008,1,3,4,754,735,1002,1000,WN,3231,N772SW,128,145,113,2,19,IAD,TPA,810,5,10,0,,0,NA,NA,NA,NA,NA
515	2008,1,3,4,628,620,804,750,WN,448,N428WN,96,90,76,14,8,IND,BWI,515,3,17,0,,0,NA,NA,NA,NA,NA

```

- MapFileCreator.java  
```java
package com.sist;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.util.Tool;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapFileOutputFormat;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.SequenceFileInputFormat;
import org.apache.hadoop.mapred.SequenceFileOutputFormat;
import org.apache.hadoop.util.ToolRunner;

public class MapFileCreator extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		JobConf conf = new JobConf(MapFileCreator.class);
		conf.setJobName("MapFileCreator");
		
		
		Configuration config = new Configuration();
		FileSystem hdfs = FileSystem.get(config);
		
		Path path = new Path(args[1]);
		if(hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		//입출력 경로
		FileInputFormat.setInputPaths(conf, new Path(args[0]));
		FileOutputFormat.setOutputPath(conf, path);

		//입력 포맷
		conf.setInputFormat(SequenceFileInputFormat.class);
		//출력 포맷 
		conf.setOutputFormat(MapFileOutputFormat.class);
		
		//출력 key
		conf.setOutputKeyClass(IntWritable.class);
		
		//압축 Format 설정
		SequenceFileOutputFormat.setCompressOutput(conf, true);
		SequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);
		SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);
		
		JobClient.runJob(conf);
		
		return 0;
	}

	public static void main(String[] args) throws Exception{
		int res = ToolRunner.run(new Configuration(), new MapFileCreator(), args);
		System.out.println("***res = " + res);
	}//--main

}

```

```
hdfs dfs -text 2008_mapfile/part-00000/data | head -10
```
  
```
18/06/07 10:26:54 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/06/07 10:26:54 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 10:26:54 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 10:26:54 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 10:26:54 INFO compress.CodecPool: Got brand-new decompressor [.gz]
11	2008,8,10,7,1315,1220,1415,1320,OH,5572,N819CA,60,60,14,55,55,JFK,LGA,11,8,38,0,,0,55,0,0,0,0
11	2008,5,15,4,2037,1800,2125,1900,OH,4988,N806CA,48,60,31,145,157,JFK,LGA,11,10,7,0,,0,145,0,0,0,0
17	2008,3,8,6,NA,1105,NA,1128,AA,1368,,NA,23,NA,NA,NA,EWR,LGA,17,NA,NA,1,B,0,NA,NA,NA,NA,NA
21	2008,5,9,5,48,100,117,130,AA,588,N061AA,29,30,11,-13,-12,MIA,FLL,21,6,12,0,,0,NA,NA,NA,NA,NA

```
소팅이 잘 되어 있다.  

- SearchValueList.java  
검색  

```java
package com.sist;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.MapFile.Reader;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapred.MapFileOutputFormat;
import org.apache.hadoop.mapred.Partitioner;
import org.apache.hadoop.mapred.lib.HashPartitioner;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/**
 * hadoop jar SearchValueList.jar com.sist.SearchValueList 2008_mapfile 100
 * @author sist
 *
 */
public class SearchValueList extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		Path path = new Path(args[0]);
		FileSystem fs = path.getFileSystem(getConf());
		
		//MapFile reading
		Reader[] readers = MapFileOutputFormat.getReaders(fs, path, getConf());
		
		//검색 key
		IntWritable key = new IntWritable();
		key.set(Integer.parseInt(args[1]));
		
		//검색 결과 저장
		Text value = new Text();
		
		//partitioner를 이용해 검색 key가 저장된 map file 조회
		Partitioner<IntWritable, Text> partitioner = new HashPartitioner<IntWritable, Text> ();
		
		Reader reader = readers[partitioner.getPartition(key, value, readers.length)];
		
		Writable entry = reader.get(key,  value);
		if(entry == null) {
			System.out.println("no key found");
		}
		
		//매 파일 순회 키와 값을 출력
		IntWritable nextKey = new IntWritable();
		do {
			System.out.println("**" + value.toString());
		} while(reader.next(nextKey, value) && key.equals(nextKey));
		
		return 0;
	}//--run
	
	public static void main(String args[]) throws Exception {
		int res = ToolRunner.run(new Configuration(), new SearchValueList(), args);
		System.out.println("***res = " + res);
	}//--main

}

```

실행하기 전에 파일 정리  
```
hdfs dfs -rm -r /user/sist/mapfile/_SUCCESS  
hadoop jar SequenceFileCreator.jar com.sist.SearchValueList 2008_mapfile 30  
```

결과  
```
18/06/07 11:14:12 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.deflate]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.deflate]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.deflate]
18/06/07 11:14:12 INFO compress.CodecPool: Got brand-new decompressor [.deflate]
**2008,1,8,2,816,805,907,855,B6,9002,N236JB,51,50,19,12,11,JFK,HPN,30,5,27,0,,0,NA,NA,NA,NA,NA
**2008,1,6,7,2226,2200,2301,2240,CO,348,N56859,35,40,11,21,26,SJC,SFO,30,7,17,0,,0,0,0,0,0,21
**2008,12,31,3,NA,1800,NA,1820,OO,5613,N579SW,NA,20,NA,NA,NA,SFO,SJC,30,NA,NA,1,B,0,NA,NA,NA,NA,NA
**2008,8,8,5,1448,1440,1602,1540,OH,5052,N442CA,74,60,23,22,8,HPN,JFK,30,9,42,0,,0,0,0,22,0,0
**2008,9,22,1,1340,1325,1553,1425,OH,6898,N710CA,133,60,27,88,15,HPN,JFK,30,12,94,0,,0,0,0,88,0,0
```

### 분산 환경에서의 정렬  
- 전체 정렬  
  - 분산처리의 장점을 살리면서 sort (어느 한 node에 정렬 작업이 몰리지 않도록!)
     - 입력 데이터를 sampling해서 데이터 분포도 조사
     - 데이터 분포도에 맞게 파티션 정보를 미리 생성 (key 고르게 분배)
     - 미래 생성한 파티션 정보에 맞게 출력 데이터를 생성
        - TotalOrderPartitioner, InputSampler  
     - 각 출력 데이터를 병합 

- SequenceFileTotalSort.java
```java
package com.sist;

import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.SequenceFileInputFormat;
import org.apache.hadoop.mapred.SequenceFileOutputFormat;
import org.apache.hadoop.mapred.lib.InputSampler;
import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class SequenceFileTotalSort extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		JobConf conf = new JobConf(getConf(), SequenceFileTotalSort.class);
		conf.setJobName("SequenceFileTotalSort");
		
		conf.setInputFormat(SequenceFileInputFormat.class);
		conf.setOutputFormat(SequenceFileOutputFormat.class);
		
		conf.setOutputKeyClass(IntWritable.class);
		conf.setPartitionerClass(TotalOrderPartitioner.class);
		
		//SequenceFile 압축 Format 설정
		SequenceFileOutputFormat.setCompressOutput(conf, true);
		SequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);
		SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);
		
		//입출력 경로
		FileInputFormat.setInputPaths(conf,  new Path(args[0]));
		FileOutputFormat.setOutputPath(conf, new Path(args[1]));
		
		Path inputDir = FileInputFormat.getInputPaths(conf)[0];
		inputDir = inputDir.makeQualified(inputDir.getFileSystem(conf));
		
		Path partitionFile = new Path(inputDir, "_partitionFile");
		
		TotalOrderPartitioner.setPartitionFile(conf, partitionFile);		

		//샘플 데이터 추출 : 10개의 입력 split에서 0.1의 확률로 1000건의 데이터를 sampling
		InputSampler.Sampler<IntWritable, Text> sampler 
		= new InputSampler.RandomSampler<IntWritable, Text> (0.1, 1000, 10);
		
		InputSampler.writePartitionFile(conf, sampler);
		
		//각 task에서 partition 정보를 참조할 수 있게, 분산 cache에 partition 정보를 등록
		URI partitionUrk = new URI(partitionFile.toString() + "#_partitions");
		
		DistributedCache.addCacheFile(partitionUrk, conf);
		DistributedCache.createSymlink(conf);
		
		JobClient.runJob(conf);
		
		return 0;
	}//--run

	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new SequenceFileTotalSort(), args);
		System.out.println("***res = " + res);

	}//--main

}

```


```
hadoop jar SequenceFileCreator.jar com.sist.SequenceFileTotalSort 2008_sequencefile 2008_totalsort
hdfs dfs -text 2008_totalsort/part-00000 | head -10
```
  
```
18/06/07 12:11:39 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
18/06/07 12:11:39 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 12:11:39 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 12:11:39 INFO compress.CodecPool: Got brand-new decompressor [.gz]
18/06/07 12:11:39 INFO compress.CodecPool: Got brand-new decompressor [.gz]
11	2008,8,10,7,1315,1220,1415,1320,OH,5572,N819CA,60,60,14,55,55,JFK,LGA,11,8,38,0,,0,55,0,0,0,0
11	2008,5,15,4,2037,1800,2125,1900,OH,4988,N806CA,48,60,31,145,157,JFK,LGA,11,10,7,0,,0,145,0,0,0,0
17	2008,3,8,6,NA,1105,NA,1128,AA,1368,,NA,23,NA,NA,NA,EWR,LGA,17,NA,NA,1,B,0,NA,NA,NA,NA,NA
21	2008,5,9,5,48,100,117,130,AA,588,N061AA,29,30,11,-13,-12,MIA,FLL,21,6,12,0,,0,NA,NA,NA,NA,NA

```

---
##### import 정리 단축키 (Eclipse)
Ctrl Shift O  
일단 import ~~~. *; 하고  
다 코딩한 후 Ctrl Shift O 누르면 정리됨  
